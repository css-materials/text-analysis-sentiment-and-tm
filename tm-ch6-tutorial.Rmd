---
title: "Chapter 6 Topic modeling tutorial"
output: github_document
---

This tutorial works through the code of [Chapter 6.1](https://www.tidytextmining.com/topicmodeling#latent-dirichlet-allocation)

For further practice, work through the code of [Chapter 6.2](https://www.tidytextmining.com/topicmodeling#library-heist) 


LOAD LIBRARIES AND DATA

Load libraries 
```{r}
library(tidyverse)
library(tidytext)
library(topicmodels)  
# install first if working on your local pc with install.packages("topicmodels")
```

Load data
```{r}
data("AssociatedPress")
```

The AssociatedPress dataset comes with the package, and consists of 2246 news articles and 10473 distinct words
```{r}
AssociatedPress
```


CONVERT AMONG DATA FORMATS

We need a document-term matrix as input to run LDA. These data are conveniently already in the right shape, but what if we had these data in a tidy-text format instead?
See https://www.tidytextmining.com/dtm.html

Use `tidy()` to convert from matrix to tidy text format (one-token-per-document-per-row), which is useful for basic exploratory analyses (see Chapter 1 and 3) and for sentiment analysis with tidy data (Chapter 2).
```{r}
ap_tidy <- tidy(AssociatedPress)
ap_tidy
```

Use `cast_dtm()` to convert tidy data back to document-term matrix format
```{r}
ap_tidy %>%
  cast_dtm(document, term, count)
```


RUN LDA

The main function is `LDA()` from `topicmodels`. The user must: 
* specify `k` (number of topics): to get started, the book picks 2 topics
* set a seed so that the output of the model is reproducible
```{r}
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda
```

As the data size and the number of topics you ask increase, the longest it takes for R to estimate the model.


EXPLORE RESULTS

The function `LDA()` returns an object containing the full details of the model.
We should always check the two probabilities distributins estimated by the model: 
* beta: per-topic-per-word probabilities
* gamma: per-document-per-topic probabilities

First, we extract the per-topic-per-word probabilities "beta”
```{r}
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```
For each topic-term we have a probability of being generated from topic 1 and 2

We can find the 10 terms most common within each topic
```{r}
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%    # can also use" top_n(10, beta) 
  ungroup() %>%
  arrange(topic, -beta)
ap_top_terms
```

We can manipulate and visualize these common terms using `dplyr` and `ggplot2`
```{r}
ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

THINK: 
Is there evidence that these words cohere into something that can be called a topic?
Or do we need to re-run the model asking for more topics?

Second, we extract the per-document-per-topic probabilities "gamma” 
```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents
```

Interpretation: 25% of words in document 1 are generated from topic 1, most of these documents are drawn from a mix of topics, but document 6 seems to be drawn almost entirely from topic 2

THINK:


Further manipulate this document-term-matrix to closely inspect most common words in document 6:
```{r}
tidy(AssociatedPress) %>%
  filter(document == 6) %>%
  arrange(desc(count))
```

